{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "giqpVs8xaBcB",
    "tags": []
   },
   "source": [
    "# **CSCI 1470 Lab 01:** Introduction to Machine Learning \n",
    "---\n",
    "***important***:\n",
    "- Before starting the lab please copy this notebook into your own google drive by clicking on \"File\" and \"Save a copy in drive\"\n",
    "- If you want to work locally, make sure that you also download lab1utils.py in the same folder as this notebook\n",
    "---\n",
    "\n",
    "In this lab, we will be introducing the basic idea behind machine learning. You will also get to build two machine learning models by yourself. In doing so, you will get familiar with the process of training an ML model from the gradients of its loss function. \n",
    "\n",
    "The goal of this all this lab is to catch students up to where everybody is on a level playing field. **we hope you find this lab useful! :-)**\n",
    "\n",
    "**Make sure to get all questions checked off by your TA to get credit for this lab!** \n",
    "\n",
    "**HINT:** \n",
    "- Search \"Check-\" in browser search (cmd+f mac, ctrl+f windows)\n",
    "- You can also open the \"table of contents\" from the side menu on the left side of your screen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qOrumFx-zOvT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "isColab = \"google.colab\" in sys.modules\n",
    "# this also works: \n",
    "# isColab = \"COLAB_GPU\" in os.environ\n",
    "\n",
    "if isColab:\n",
    "    from google.colab import drive \n",
    "    drive.mount(\"/content/drive\", force_remount=True) \n",
    "\n",
    "    # TODO: write the path to your copy of the lab\n",
    "    colab_path = (\"/content/drive\" \n",
    "        + \"/Shared drives/CS1470 TAs Fall 2022/Labs/lab01_ml\")\n",
    "    sys.path.append(colab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "izl7PUiAzOvU"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lab1utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# import plotly.graph_objects as go\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# from sklearn.linear_model import LinearRegression\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# import itertools\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# or that its path is included in the system path variable.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# lab1utils.py is a bunch of visualization stuff that you don't need to know right now\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlab1utils\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lab1utils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# import itertools\n",
    "\n",
    "# make sure that lab1utils.py is either in the same folder as this notebook \n",
    "# or that its path is included in the system path variable.\n",
    "# lab1utils.py is a bunch of visualization stuff that you don't need to know right now\n",
    "import lab1utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lpGxOpliaBcC",
    "tags": []
   },
   "source": [
    "## Conceptual Background\n",
    "\n",
    "### The Motivation for Machine Learning\n",
    "\n",
    "In many previous courses, you have had to implement various algorithms to find \"solutions\". For example: \n",
    "- You'd like to take a user's math equation and spit out a calculator's solution. \n",
    "- You'd like to take a graph specification and output the shortest path from one location to another. \n",
    "- You want a user to run your program and get the text \"Hello World!\" in the console.\n",
    "\n",
    "There are many clever algorithms out there that are able to do a lot of things, both very specific and very general. However, there are some problems that may be a bit too hard to code up a good solution for. For example, **how do you identify whether an image is a cat or a dog?** Humans can definitely do it, since you've seen many examples of animals and probably have an internal understanding of what they look like. But how would a computer, which merely has access to a large space of pixels, do it? \n",
    "\n",
    "It turns out that **machine learning** is a great way of developing such algorithms, and can be applied when we have examples to train off of. The basic paradigm shift can be described as follows: \n",
    "\n",
    " - **Normal Computation:** Inputs and Function -> Outputs\n",
    " - **Machine Learning:** \n",
    "   - **Supervised:** Inputs and Outputs -> Function that approximately maps the inputs to the outputs\n",
    "   - **Unsupervised:** Inputs -> Associations and patterns of the input data\n",
    "\n",
    "This course will really mostly focus on supervised machine learning formulations in the earlier parts, so we're just going to introduce this for now. In supervised machine learning, we have a lot of data with various properties. Some of the properties are easy to get, while others are not so simple. The idea is that we want to predict some of the harder properties in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOmGGymIJ6wH"
   },
   "source": [
    "### A Simple Problem Formulation\n",
    "\n",
    "Let's imagine that you have this magical function that can tell if the animal in the image is a cat or a dog $f: \\mathbf{X} \\mapsto Y$:\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbf{x} &\\in  \\{ 0, 1, \\cdots, 255 \\}^{H \\times W \\times 3} \n",
    "    & \\text{ (pixel values of an image)}\\\\\n",
    "y &\\in \\{\\text{cat}, \\text{dog}\\} \n",
    "    & \\text{ (true label of an image)} \\\\\n",
    "f(\\mathbf{x}) &= y \n",
    "    & \\text{ (true function from image to label)}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "**Supervised Learning** \n",
    "- **Goal:** Discover the function $y = f(\\mathbf{x})$ so you can use it. \n",
    "- **Problem:** $f$ usually cannot be found analytically or is too complicated.\n",
    "- **Solution:** Find an **approximation** $\\hat{y} = h(\\mathbf{x})$ that is similar to $y$.\n",
    "\n",
    "- **Limitations:**\n",
    "    - The data is often-times generated from the real world, so we have limited observations. \n",
    "        - **Solution**: Train $h$ on a subset of known data, and then see how it performs on held-out a testing subset of data. \n",
    "    - We might have no idea of the underlying data relationships.\n",
    "        - **Solution:** We make an assumption on kind of function $h$ could be and test out our hypothesis by fitting and evaluating it. \n",
    "- **Examples:** linear regression, quadratic discriminant analysis, k-nearest neighbors, support vector machine, random forest, etc\n",
    "\n",
    "**Unsupervised Learning**\n",
    "+ The goal is to find the associations and patterns in a set of input variables $\\{\\mathbf{x}_{1}, \\mathbf{x}_{2}, \\cdots, \\mathbf{x}_{n} \\}$.\n",
    "+ There is no target variable $y$; instead, we have target function classes, such as a clustering function.\n",
    "+ **Examples:** principal component analysis, k-means clustering, etc\n",
    "\n",
    "In this lab, we will begin with the supervised learning algorithms, which is a bit easier to understand conceptually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCWvMnoWt35T",
    "tags": []
   },
   "source": [
    "### Loss Functional\n",
    "\n",
    "**How do we actually find the prediction function $h: \\mathcal{X} \\to \\mathcal{Y}$ in supervised learning?**\n",
    " + We find the **target function** that minimizes the **loss functional** \n",
    " + The **loss functional** $L: \\mathcal{H} \\to \\mathbb{R}$ measures the distance between the model $h$ and the ground truth $f$. \n",
    "    + $\\mathcal{H}$ is the space of all candidate functions. \n",
    "        + In theory, these can be anything. However, in practice people make assumptions about what these functions can be and bind them to forms that are tuneable by some parameters $\\theta$. As a very simple example, $\\mathcal{H}$ could be defined by the function $h_\\theta(x) = x\\theta$ and reduce their search space to a selection of $\\theta$.\n",
    "    + The target function is the member $h$ that minimizes the distance $\\mathrm{argmin}_{h \\in \\mathcal{H}}{L(h)}$, and is the closest approximation to the true $f$.\n",
    "        + If we reduce our search space to $h_\\theta$, this becomes $\\mathrm{argmin}_{\\theta}L(h_\\theta)$.\n",
    " + The goal is to make $\\hat{y} = h_{\\pmb{\\theta}}(\\mathbb{x})$ as close to $y = f(\\mathbf{x})$ by tweaking the parameters $\\pmb{\\theta} = \\{\\theta_1, \\theta_2, \\cdots, \\theta_n\\}$. \n",
    "\n",
    "Here is overall process:\n",
    "1. Construct a model that incorporates the prediction function $h_{\\pmb{\\theta}}$.\n",
    "2. Keep tweaking the parameters $\\pmb{\\theta} = \\{\\theta_1, \\theta_2, \\cdots, \\theta_n\\}$.\n",
    "3. Find the set of parameters that minimizes the loss functional $L(h_{\\pmb{\\theta}})$.\n",
    "4. Evaluate the performance of the model with a new set of data that the model has not seen yet. \n",
    "\n",
    "**However, you don't really have a method of evaluating the loss functional in the exact analytic form**. \n",
    " + You only have a finite number of the input-target variable pairs $\\{(\\mathbf{x}_{1}, y_{1}), (\\mathbf{x}_{2}, y_{2}), \\cdots, (\\mathbf{x}_{n}, y_{n}) \\}$\n",
    " + Thus, you calculate the **empirical loss function** $\\mathbb{E}[L(\\theta, \\mathbf{x}, y)]$ instead. \n",
    " + Remember that the values of $\\mathbf{X}$ and $y$ are fixed, because they are members of the training dataset $\\mathbb{X}_{train}, \\mathbb{Y}_{train}$\n",
    " + The empirical loss function is only a function of the parameters $\\theta$. \n",
    " + **Then the problem is how to find the optimal set of parameters $\\theta$ that minimizes the empirical loss function**. \n",
    "\n",
    "Here are some examples of empirical loss functions:\n",
    " - In a regression problem\n",
    "   - The empirical loss function is often the mean squared error $L(h) = \\mathbb{E}[(h(X) - Y)^2]$.\n",
    "   - The target function is the regression function $r(\\mathbf{x}) = \\mathbb{E}[Y | \\mathbf{X} = \\mathbf{x}]$. \n",
    " - In a classification problem:\n",
    "   - The empirical loss function is often the misclassification probability $L(h) = \\mathbb{P}[h(\\mathbf{X}) \\neq Y]$.\n",
    "   - The target function is $G(\\mathbf{x}) = \\mathrm{argmax}_{c} \\mathbb{P}[Y = c | \\mathbf{X} = \\mathbf{x}]$. \n",
    " \n",
    "\n",
    "We may also choose to incorporate a validation dataset. The formal role distribution is: \n",
    "- Use $\\mathbb{X}_{train}, \\mathbb{Y}_{train}$ to directly inform $\\theta$.\n",
    "- Use $\\mathbb{X}_{valid}, \\mathbb{Y}_{valid}$ to suggest generalizability (and indirectly inform architecture of $h$).\n",
    "- Use $\\mathbb{X}_{test}, \\mathbb{Y}_{test}$ to suggest generalizability (and verify that validation dataset didn't screw up model selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BzNl2HpSJ6wI"
   },
   "source": [
    "### Optimization and Gradient Descent\n",
    "\n",
    "If the loss function $L(\\theta, \\mathbb{X}_{train}, \\mathbb{Y}_{train})$ is simle enough, then it is easy to minimize it. We simply find all the points where the partial derivative $\\frac{\\partial L}{\\partial \\theta} = 0$ and take the smallest one as the global minimum. \n",
    "\n",
    "- **Problem:** In many cases and certainly in deep learning, the loss function is too complicated and it is practically impossible to solve $\\frac{\\partial L}{\\partial \\theta} = 0$. \n",
    "- **Solution:** If we can evaluate $\\frac{\\partial L}{\\partial \\theta}$, the derivative of loss with respect to the tuneable parameters $\\theta$, we can train our model to approach a minimum. \n",
    "\n",
    "**What do we do when the loss function is too complicated?** \n",
    "- We use the technique called the **gradient descent**. \n",
    "- We begin from a random set of parameters $\\theta_{\\text{initial}}$\n",
    "- We repeat updating the parameters with the **gradient of the loss function** $\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\frac{\\partial L}{\\partial \\theta}$. \n",
    "- Here $\\eta$ is a carefully-chosen number called the **learning rate**. \n",
    "  \n",
    "This method is what we use to train more complicated machine learning models such as the deep learning models.\n",
    "\n",
    "We will revisit gradient descent with actual examples later in this Notebook, but for now, here is the takeaway.\n",
    "> The gradient of the loss as a function of input, output, and prediction!\n",
    "\n",
    "Here is how gradient descent works.\n",
    "+ If the gradient is **positive** (${L}$ and $\\theta$ are **positively correlated** at current configuration), ***decreasing $\\theta$ decreases ${L}$***.\n",
    "+ If the gradient is **negative** (${L}$ and $\\theta$ are **negatively correlated** at current configuration), ***increasing $\\theta$ decreases ${L}$***.\n",
    "\n",
    "So... **why not just shift $\\theta$ by the negative gradient?** When the parameter setting has a lot of impact on the loss w/ current param configurations, its gradient will have a large magnitude and so it will be shifted significantly and vice versa. This seems like a good plan (and is the simplest version of gradient descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iinFWrAiZyAW"
   },
   "source": [
    "### Supervised Learning Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvVpxB_OuR9h"
   },
   "source": [
    "<details><summary><b>Click here for more examples of supervised learning problems</b></summary>\n",
    "\n",
    "- **Predict position of a ball after $t$ seconds.**\n",
    "  - $h : \\mathbb{R} \\to \\mathbb{R}$. Input might be lower-bounded.\n",
    "  <details><summary><b>Basic Regression</b></summary> $\\mathbb{R}$ is the set of all real numbers, which include positive and negative decimals and irrational numbers (i.e. $\\pi, e, \\sqrt{2}$). This is a natural regressional (numerical) formulation: The output is a numerical value that represents some continuous distribution. </details>\n",
    "  - Maybe minimize Euclidean Distance $\\mathcal{L}(h, x, y) = ||h(x) - y||_2$. \n",
    "\n",
    "+ **Predict the job type (out of 8 possible jobs) of employees based on the pay, number of years worked, and years in school** \n",
    "  + $h : \\mathbb{R}^3 \\to [0,1]^8$\n",
    "  <details><summary><b>One-Hot Encoding Representation</b></summary> A set raised to a power $n$ is set of $n$-element combinations of the set. For example, $\\{0,1\\}^2 = \\{(0,0), (0,1), (1,0), (1,1)\\}$ where each pair of numbers is technically a tuple. The problem with 8 arbitrary job types is that there is not a natural progressive relationship among them. Let's say that for a particular example, your model predicts job 3 (mechanic), and the right answer is job 7 (programmer). Does increasing your prediction to job 3.5 or job 4 (a secretary) make your prediction any better? No. However, let's say that you're now associating each class as a probability and are predicting $[0, 0, 0.9, 0, 0, 0, 0.1, 0]$. We can then try to transition this into a correct prediction pretty easily without any cross-category conflicts. </details>\n",
    "  + $h$ probably contains softmax to conform to discrete probability distribution.\n",
    "  + Consider using $\\mathcal{L}(h, x, y) = \\text{CE}(h(x), y)$.\n",
    "\n",
    "- **Predict whether an image contains a cat or a dog.** \n",
    "  - $h : \\{0,\\cdots,255\\}^{H \\times W \\times 3} \\to [0, 1]$\n",
    "  <details> <summary><b>BIG Input Space</b></summary> Note how the image space is very big. For a 100x100 image with 3 channels (and 255 intensity options per channel), that's ~30 million possible inputs. </details>\n",
    "  <!-- - $h : \\{0,\\cdots,255\\}^{H \\times W \\times 3} \\to \\sigma(\\mathbb{R}^2) = [0, 1]$ for softmax function $\\sigma$ -->\n",
    "\n",
    "<!-- - Colorize a greyscale photo based on a reference pic.  -->\n",
    "  <!-- - $h : \\{0,\\cdots,255\\}^{H \\times W \\times (3+1)} \\to \\{0,\\cdots,255\\}^{H \\times W \\times 3}$ -->\n",
    "\n",
    "+ **Pick a move for a robot that can make one of 10 motions**. \n",
    "  + $h : \\text{State Abstraction Space} \\to [0,1]^{10}$\n",
    "  <details> <summary><b>What's The Input Space?</b></summary> Some problems make it really hard to define an input space. What level of abstraction would you even use? For chess, do you want to use a grid representation? Maybe a graph representation? How about the image representation when viewed from a particular camera? </details>\n",
    "<!-- - Generate the next word in a sentence after $n$ words.  -->\n",
    "  <!-- - $h : D^n \\to D$ for a dictionary $D$ with a lot of english words.  -->\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v6c1IBkYt9N9",
    "tags": []
   },
   "source": [
    "### Further Notes on Supervised ML\n",
    "\n",
    "<details> <summary> <b> Click here if you need more nerdy details </b> </summary>\n",
    "\n",
    "- In the above discussions, $x$ is an element of the set $\\mathbb{X}$ which is a set of realizations of the distribution $\\mathcal{X}$ (and the same goes for $y$ etc.). However, we will begin referring to $\\mathbb{X}$ and $\\mathbb{Y}$ as vectors when it becomes useful (this is how they are stored in memory anyways).\n",
    "\n",
    "- $h(x) = \\hat{y}$ is a common and convenient notation. These symbols can be called the 'prediction' or the 'hypothesis'.\n",
    "\n",
    "- $\\mathbb{Y}$ can be referred to as the \"ground truth\"... even though it could have plenty of elements that are misrepresentative or low-quality...\n",
    "\n",
    "- $\\mathbb{X, Y} \\subset \\mathcal{D}(\\mathbb{X}), \\mathcal{D}(\\mathbb{Y})$ : This is trivially true, but it's worth mentioning. The observations $\\mathbb{X}$ and $\\mathbb{Y}$ are a subset of all possible inputs/outputs that span over some mathematical domain.\n",
    "\n",
    "- $\\mathbb{X, Y} \\sim \\mathcal{X, Y}$ : $X$ and $Y$ come from of some probability distributions $\\mathcal{X,Y}$.\n",
    "\n",
    "- $\\mathcal{X}$ and $\\mathcal{Y}$ are interdependent : $\\mathcal{X}$ and $\\mathcal{Y}$ depend on each other such that $\\mathcal{Y}$ can be predicted by $\\mathcal{X}$ with some amount of confidence.\n",
    "\n",
    "- The true function $f: \\mathcal{X} \\to \\mathcal{Y}$ such that $f(\\mathbb{X}) = \\mathbb{Y}$ is an assumption that we can make from the interdependence between $\\mathcal{X}$ and $\\mathcal{Y}$. In reality, whenever you are making an observation, you are drawing samples from the joint distribution $\\mathcal{X} \\times \\mathcal{Y}$. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IT6yyYjSvIGV"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pE9RPNZtzOvZ"
   },
   "source": [
    "## Regression Problem: Associating Inputs and Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCy9auyXzOvZ"
   },
   "source": [
    "Enough of theory. Let's actually do something! \n",
    "\n",
    "For this exercise, we will be modeling a basic **regression task**; that is, the task of finding an interdependance between two sets of values: \n",
    "- **Independent Variables**: The *inputs* that are pulled from some space. \n",
    "- **Dependent Variables**: The *outputs* that depend on the inputs.\n",
    "\n",
    "**For this task, we will do the following:**\n",
    "- Create a toy dataset with known distribution to train our models on.\n",
    "- Formulate the mean function as a 'model' and get a sense for the lingo.\n",
    "- Make a basic ***linear regression*** model to show the basics.\n",
    "- Extend the model to reason with ***generalized regression*** problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiHDdRudzOva"
   },
   "source": [
    "### Toy Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTPw3zOWFzgw"
   },
   "source": [
    "Every machine learning project starts with collecting data, and that's also where we will begin. Real data relationships are extremely complex and usually not known, but in general associated variables have two components:\n",
    "- **Dependence** $\\mathbb{E}[Y | X = \\mathbf{x}]$: the components of $Y$ that is explained by $X$.\n",
    "- **Noise** $\\xi = p - \\mathbb{E}[Y | X = \\mathbf{x}]$: The components of $Y$ that is **not** explained by $X$. \n",
    "\n",
    "We will make some synthetic data, making sure to maintain both dependance and noise by generating from random distributions.\n",
    "\n",
    "- Let $x_1, x_2$ be generated from a multi-normal distribution with non-diagonal covariance matrix.\n",
    "    - Simply put, this makes $x_1$ and $x_2$ somewhat interdependent; a diagonal cov matrix would make them independent of each other. \n",
    "- Let $y$ be generated from a normal distribution with mean $\\mu_y$ being a complex function of $x_1$ and $x_2$ and standard deviation $\\sigma_y \\neq 0$.\n",
    "    - This means that $\\mathbb{E}[Y | X = \\mathbf{x}]$ is a complex function which gets obscured by random noise quantity $\\xi \\sigma_y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Zsr_DGMzOva"
   },
   "source": [
    "\\begin{align*}\n",
    "(x_1, x_2) & \\sim \\mathrm{MultiNormal}(\n",
    "\\mu = [1, 2], \\,\n",
    "\\mathrm{cov} = \n",
    "\\begin{bmatrix}\n",
    "6 & 3 \\\\\n",
    "3 & 6\n",
    "\\end{bmatrix}\n",
    ") \\\\\n",
    "\\\\ \n",
    "y & \\sim \\mathrm{Normal}(\\mu = \\mu_y,\\, \\sigma = \\sigma_y) \\\\\n",
    "& \\mu_y = 0.8x_1 + 0.2x_2 - 0.05x_{1}^2 - 0.04x_{1}x_{2} + 20 \\\\\n",
    "& \\sigma_y = 2 \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NaTl6TWPzOva"
   },
   "source": [
    "#### [Check-off #1] Generate Data\n",
    "- Read the NumPy API guide on the [multivariate normal generator](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.multivariate_normal.html) and complete the `generate_X` function below.\n",
    "- Read the NumPy API guide on the [normal generator](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.normal.html) and complete the `generate_y` function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EK8cLP0UzOva"
   },
   "outputs": [],
   "source": [
    "## DO NOT CHANGE ANY OF THE DEFAULT VALUES\n",
    "def generate_X(n_obs, seed=42):\n",
    "    \"\"\"Draws random samples from the parameterized 2D multi-normal distribution\"\"\"\n",
    "    mean = (1, 2)\n",
    "    cov = ((6, 3), (3, 6))\n",
    "\n",
    "    rng = np.random.default_rng(seed = seed)                    \n",
    "    X = rng.multivariate_normal(??, ??, size = ??)              ## TODO\n",
    "    return X\n",
    "    \n",
    "def generate_y(X, seed=42): \n",
    "    \"\"\"Draws random samples from the parameterized normal distribution\"\"\"\n",
    "    weights = (20.0, 0.8, 0.2, -0.05, -0.04, 0.0)\n",
    "    stdev = 1.0\n",
    "\n",
    "    b0, b1, b2, b11, b12, b22 = weights                         \n",
    "    x1, x2 = X[:, 0], X[:, 1]                                   \n",
    "    mean = b0 + b1*x1 + b2*x2 + b11*(x1**2) + b12*(x1*x2)       \n",
    "    \n",
    "    rng = np.random.default_rng(seed = seed)                    \n",
    "    y = rng.normal(loc = ??, scale = ??).reshape(-1, 1)         ## TODO\n",
    "    return y\n",
    "\n",
    "X = generate_X(300)\n",
    "print(f\"shape of inputs: {X.shape}\")\n",
    "print(f\"first five inputs: \\n{X[:5]}\\n\")\n",
    "\n",
    "y_true = generate_y(X)\n",
    "print(f\"shape of outputs: {y_true.shape}\")\n",
    "print(f\"first five outputs: \\n{y_true[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FWKt5Z4deUp"
   },
   "source": [
    "The result should look like this.\n",
    "```\n",
    "shape of inputs: (300, 2)\n",
    "first five inputs: \n",
    "[[ 1.62731266  0.07988226]\n",
    " [-1.7438992   1.56000442]\n",
    " [ 6.73360831  4.54393296]\n",
    " [ 1.11612605  1.34149306]\n",
    " [ 2.08040181  0.99087946]]\n",
    "\n",
    "shape of outputs: (300, 1)\n",
    "first five outputs: \n",
    "[[21.48363666]\n",
    " [17.86086272]\n",
    " [23.24919715]\n",
    " [21.96461353]\n",
    " [19.5919872 ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYyERZTfzOvb"
   },
   "source": [
    "We can visualize the data in the 3D scatter plot here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDci46QszOvc"
   },
   "outputs": [],
   "source": [
    "fig_data = lab1utils.visualize_data(X, y_true)\n",
    "fig_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vujqrcijzOvc"
   },
   "source": [
    "**Please feel free to drag the 3D plot with your mouse pointer and inspect the data from various angles**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9nbkvHmzOvc"
   },
   "source": [
    "### Mean Predictor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwWF1Kw_zOvc"
   },
   "source": [
    "So, now that we have our data all gathered up, let's make the simplest model possible. \n",
    "\n",
    "In this case, we already know the true distribution $\\mathcal{X} \\times \\mathcal{Y}$, so we don't even need machine learning in the first place. However, we'll pretend that we have no idea how this data was generated because – in many real cases – **we don't know the underlying distribution of our data.** All we have is some number of realizations $\\mathbb{X}$ and $\\mathbb{Y}$ pulled from the distribution. \n",
    "\n",
    "The best that we can do in real cases is to make the assumption that there is some interdependence between $\\mathbb{X}$ and $\\mathbb{Y}$, and try to find the relation $y = f(x_1 , x_2)$ by using machine learning. Hopefully, if we do it right, our prediction function $h(x_1 , x_2)$ will have some predictive power that generalizes from our limited observable set to the set of unobserved points that will be encountered later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqV0JhBezOvc"
   },
   "source": [
    "As a baby step, let's first make the mean predictor model, which will always return the same number no matter the values of the independent variables $\\mathbf{x}$, and that number is going to be the mean of all target variable $y$ in the training dataset. Just look at the code cell below and see how ridiculously simple the model is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLZEcyEmzOvc"
   },
   "outputs": [],
   "source": [
    "class MeanPredictor:\n",
    "    def __init__(self):\n",
    "        self.mean = 0\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"mean predictor model, mean = {self.mean}\"\n",
    "\n",
    "    def __call__(self, X):\n",
    "        y_pred = self.mean * np.ones_like(X[:, [0]])\n",
    "        return y_pred\n",
    "    \n",
    "    def fit(self, X, y_true):\n",
    "        self.mean = np.mean(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waXuHXmOzOvd"
   },
   "source": [
    "The mean of the target variable $y$ in the training data is about 20.578 and that's what this model always returns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fcfo3sWkzOvd"
   },
   "outputs": [],
   "source": [
    "mp_model = MeanPredictor()\n",
    "mp_model.fit(X, y_true)\n",
    "X_checkoff = np.array([[1, 1], [2, 3], [1.5, 2]])\n",
    "print(f\"mp_model(X_checkoff) = \\n{mp_model(X_checkoff)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbjzVlhvzOvd"
   },
   "source": [
    "This model is your \"Hello, World\" to machine learning. Just like \"Hello, World\", the goal is not to understand the model itself, but to understand everything else around it. In this case, that \"everything else\" is going to be the interface to interact with the model. \n",
    "\n",
    "Here, the `ModelWrapper` is a wrapper around a model to make interacting with the model easier, and by easier we mean that you can interact with the model with fewer lines of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mnt_7fjDzOvd"
   },
   "outputs": [],
   "source": [
    "class ModelWrapper:\n",
    "\n",
    "    _loss_dict = {\n",
    "        \"MAE\" : lambda y_true, y_pred: np.mean(np.abs(y_true - y_pred)),        # mean abosolute error\n",
    "        \"MSE\" : lambda y_true, y_pred: np.mean((y_true - y_pred)**2),           # mean squared error\n",
    "        \"RMSE\": lambda y_true, y_pred: np.sqrt(np.mean((y_true - y_pred)**2)),  # root mean squared error\n",
    "    }\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"wrapper around the following model:\\n{self.model}\"\n",
    "    \n",
    "    def _get_loss_fn(self, loss_key): ## <-- NOT IMPORTANT!!\n",
    "        '''If loss_key is a function, return it. \n",
    "            Otherwise, retrieve the loss function by keyword (i.e. 'MAE', 'MSE')\n",
    "        '''\n",
    "        if isinstance(loss_key, str):   return self._loss_dict[loss_key]\n",
    "        elif callable(loss_key):        return loss_key\n",
    "        else: raise ValueError('Loss function/key must be specified')\n",
    "\n",
    "    def loss(self, X, y_true, loss=\"MSE\"):\n",
    "        y_pred = self.model(X)\n",
    "        loss_fn = self._get_loss_fn(loss)\n",
    "        return loss_fn(y_true, y_pred)\n",
    "    \n",
    "    def train(self, X, y_true, **kargs):\n",
    "        train_return = self.model.fit(X, y_true, **kargs)\n",
    "        return train_return\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWQ-kcL3zOvd"
   },
   "source": [
    "The benefit of using a wrapper is that you can use the model with fewer lines of code, and the interface that the wrapper provides will always be the same no matter how complicated your model is. This benefit might not be obvious to you with a simple mean-predicting model like this one, but it will be handy if you use a more complicated model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KIxFJT8jzOvd"
   },
   "outputs": [],
   "source": [
    "# new instance, not yet trained\n",
    "mp_model2 = MeanPredictor()\n",
    "mp_model_wrapper = ModelWrapper(mp_model2)\n",
    "print(f\"Prediction before training = \\n{mp_model_wrapper.predict(X_checkoff)}\\n\")\n",
    "\n",
    "# loss before training\n",
    "print(f\"Loss before training = {mp_model_wrapper.loss(X, y_true)}\\n\")\n",
    "\n",
    "# train\n",
    "mp_model_wrapper.train(X, y_true)\n",
    "print(f\"Model is trained\\n\")\n",
    "\n",
    "# Prediction after training\n",
    "print(f\"Prediction after training = \\n{mp_model_wrapper.predict(X_checkoff)}\")\n",
    "\n",
    "# loss after training\n",
    "print(f\"Loss after training = {mp_model_wrapper.loss(X, y_true)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNAFe62CzOve"
   },
   "source": [
    "You can see that the loss is significantly smaller after the training. Now let's visualize our model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BOXrjUsJzOve"
   },
   "outputs": [],
   "source": [
    "fig_mean = lab1utils.visualize_data(X, y_true)\n",
    "go_mean = lab1utils.get_mean_go(mp_model2.mean)\n",
    "fig_mean.add_trace(go_mean)\n",
    "fig_mean.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPxs4tHUzOve"
   },
   "source": [
    "Now you must feel like you can do better than this, and you definitely can! Just not with this architecture.\n",
    "\n",
    "This mean predictor model has only one parameter $m$, and by setting it equal to the mean of all $y$ values in the training set, you have already minimized the mean squared loss. \n",
    "\n",
    "The true optimal function is $f(x) = \\mathbb{E}[Y | X = \\mathbf{x}] = \\mu_y(x)$. Recall that this is a very complex function, so... how can we go about fitting a more complex function – maybe one that assumes that $y$ is a function of $x$ – to this kind of data?\n",
    "\n",
    "<!-- If you want a better performance from your model, you have to change its very architecture, because you cannot make any further improvements by changing its parameter.  -->\n",
    "\n",
    "<!-- \\begin{align*}\n",
    "\\mathrm{MSE}(m) &= \n",
    "\\frac{1}{n}\\sum_{i=1}^{n}{(y_{\\text{true}}^{(i)} - y_{\\text{pred}}^{(i)})^2} \\\\\n",
    "&=\n",
    "\\frac{1}{n}\\sum_{i=1}^{n}{(y_{\\text{true}}^{(i)} - m)^2} \\\\\n",
    "\\end{align*} -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOHPhij2zOve"
   },
   "source": [
    "## Intro to Linear Models\n",
    "\n",
    "In our mean model:\n",
    "- There was only one parameter, which was extremely easy to set.\n",
    "- Any particular configuration of it will always output just a single value regardless of the input. \n",
    "\n",
    "We will need to go to more powerful models with more parameters if we want to actually learn more powerful correlations, and this is where the more standard regression models come in. \n",
    "\n",
    "A standard ***linear regression*** model makes the assumption that some linear scaling of the inputs contributes directly to the output. In other words, for some vector of weights $\\theta$:\n",
    "> $\\mathbb{E}[Y | X = \\mathbf{x}] \\text{ is apporimately } \\hat y = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\cdots$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ustyt4hyzOve"
   },
   "source": [
    "Let's use the exact same toy dataset again. The regression function would be the following.\n",
    "\n",
    "> $r(\\mathbf{x}) = \\mathbb{E}[Y | X = \\mathbf{x}] =  0.8x_1 + 0.2x_2 - 0.05x_{1}^2 - 0.04x_{1}x_{2} + 20$\n",
    "\n",
    "You may notice that the standard linear regression model still isn't powerful enough to represent the true distribution's regression. If you fit a linear regression model on this data, you'll actually get something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VCnpDcpzOvf"
   },
   "outputs": [],
   "source": [
    "# fitting linear regression model using the provided lab1utils library\n",
    "coef_optimized, bias_optimized = lab1utils.linreg_answer_key(X, y_true)\n",
    "go_answer_key = lab1utils.get_linreg_go(coef_optimized, bias_optimized)\n",
    "\n",
    "# visualization\n",
    "fig_linreg = lab1utils.visualize_data(X, y_true)\n",
    "fig_linreg.add_trace(go_answer_key)\n",
    "fig_linreg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzMaVJPktlhS"
   },
   "source": [
    "Obviously, the linear regression model does not fully capture the non-linearity in the data, but it is doing a pretty good job in the more densely populated areas, and it is so much better than the mean predictor model. Now, let's build a linear regression model by ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ozx8iGBjzOvf"
   },
   "source": [
    "### Linear Regression Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPeiNONfzOvf"
   },
   "source": [
    "Now let's make our own implementation of the linear regression algorithm. As you have probably learned before, linear regression assumes that the observed target variables is a random draw from the normal distribution, of which the mean is given by an affine function of the independent variables and the standard deviation is an unknown parameter.\n",
    "\n",
    "\\begin{align*}\n",
    "\\mu &= \\beta_{0} + \\beta_{1} x_{1} + \\beta_{1} x_{2} \\\\\n",
    "y &\\sim \\mathrm{Normal}(\\mu, \\sigma)\n",
    "\\end{align*}\n",
    "\n",
    "The goal of linear regression is to find the best set of coefficients ($\\beta_1$ and $\\beta_2$) and bias ($\\beta_0$), and there are several different ways to do it, in both the frequentist and the Bayesian approaches. Here, we will do it with gradient descent, which is quite an unusual way method to fit a linear regression model, but it would be a good preparation for the deep learning models later in the course. Right now, don't worry about the methods `get_gradients`, `gradient_descent` and `optimize`, and just implement the `__call__` method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vV76OR4zOvf"
   },
   "source": [
    "#### [Check-off #2] Implement the `__call__` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bv5jQzsGzOvf"
   },
   "outputs": [],
   "source": [
    "class LinearRegression_v1:\n",
    "    def __init__(self, n_vars, seed = None):\n",
    "        self.coef = np.zeros(n_vars).reshape(-1, 1) # Initialize a np array of shape (n_var, 1)\n",
    "        self.bias = np.float64(0.0) \n",
    "        # Did you know that np.float64 has many more convenient methods\n",
    "        # that vanila Python float does not have?\n",
    "        \n",
    "        if seed: # equivalent to \"if seed is not None\", but shorter and faster\n",
    "            rng = np.random.default_rng(seed = seed)\n",
    "            self.coef = rng.normal(self.coef, scale = 1.0)\n",
    "            self.bias = np.float64(rng.normal(self.bias, scale = 1.0))\n",
    "            \n",
    "    def __str__(self):\n",
    "        coef_string = np.array2string(self.coef, \n",
    "                                      precision = 6, \n",
    "                                      prefix = \" - coefficients = \")\n",
    "        return (f\"linear regression model\\n\" + \n",
    "                f\" - coefficients = {coef_string}\\n\" + \n",
    "                f\" - bias = {self.bias:.6f}\")\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \"\"\"\n",
    "        TODO: Implement the call function. \n",
    "        - use np.matmul() to directly implement the matrix formulation in the LaTeX cells below\n",
    "        - you can alternatively use np.dot() or np.sum(), but it is not really a direct \n",
    "          implementation of the matrix formulation below and we will secretly judge you.\n",
    "        - you are not allowed to use any explicit loop statements like a for-loop or a while-loop.\n",
    "        \"\"\"\n",
    "        y_pred = ??                                     ## TODO\n",
    "        return y_pred\n",
    "    \n",
    "    def get_gradients(self, X, y_true):\n",
    "        \"\"\"you will implement it later\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def gradient_descent(self, X, y_true):\n",
    "        \"\"\"you will implement it later\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y_true):\n",
    "        \"\"\"it will be given to you later\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2u5qZCozOvf"
   },
   "source": [
    "Since we are implementing in Python, we will want to take advantage of matrix operations to speed up computation by vectorizing our solution. Libraries like numpy will create pre-compiled utilities that perform matrix operations in a lower-level language than Python, which allows extremely fast iteration and good parallelization. \n",
    "\n",
    "To help us facilitate this, the following operation scheme will be used. \n",
    "\n",
    "<!-- In our model, we would like to be able to make predictions in a batch by using matrix operations. \n",
    "By \"batching\", we mean that:\n",
    "- **We do not** want to calculate the prediction on the sets of input variables one input-pair at a time, i.e. computing $\\hat{y}^{(1)} = \\beta_{0} + \\beta_{1} x_{1}^{(1)} + \\beta_{2} x_{2}^{(1)}$, then $\\hat{y}^{(2)}$, and so on. \n",
    "- **Rather, we would like to calculate the predictions $\\hat{y}^{(1)}, \\hat{y}^{(2)}, \\cdots, \\hat{y}^{(n)}$ all at once** from all observations of input variables $\\left\\{ (x_{1}^{(1)}, x_{2}^{(1)}),\\, (x_{1}^{(2)}, x_{2}^{(2)}),\\, \\cdots \\, (x_{1}^{(n)}, x_{2}^{(n)}) \\right\\}$ together. \n",
    "\n",
    "The way that we will batch the data is through the matrix formulation, for which different people use slightly different versions, but the version that we are going to use is the following.  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMNDzABQzOvf"
   },
   "source": [
    "\\begin{align*}\n",
    "\\textrm{coefficients}: \\pmb{\\beta} = \\begin{bmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\beta_2\n",
    "\\end{bmatrix} \\quad\n",
    "\\textrm{bias}: \\beta_0 \\quad\n",
    "\\textrm{inputs}&: \\mathbf{X} = \\begin{bmatrix}\n",
    "x_{1}^{(1)} & x_{2}^{(1)} \\\\ \n",
    "x_{1}^{(2)} & x_{2}^{(2)} \\\\ \n",
    "\\vdots & \\vdots \\\\ \n",
    "x_{1}^{(n)} & x_{2}^{(n)}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2fSHRC5zOvg"
   },
   "source": [
    "\\begin{align*}\n",
    "\\textrm{predictions}: \\mathbf{Y_\\text{pred}} &= \\mathbf{X} \\pmb{\\beta} + \\beta_{0} \\mathbf{1}\n",
    "\\ \\ =\\ \\ \\begin{bmatrix}\n",
    "x_{1}^{(0)} & x_{2}^{(0)} \\\\ \n",
    "x_{1}^{(1)} & x_{2}^{(1)} \\\\ \n",
    "\\vdots & \\vdots  \\\\ \n",
    "x_{1}^{(n)} & x_{2}^{(n)}\n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix}\n",
    "\\beta_{1} \\\\ \n",
    "\\beta_{2} \\\\ \n",
    "\\end{bmatrix} +\n",
    "\\begin{bmatrix}\n",
    "\\beta_{0} \\\\ \n",
    "\\beta_{0} \\\\ \n",
    "\\vdots \\\\ \n",
    "\\beta_{0}\n",
    "\\end{bmatrix} \n",
    "\\ \\ =\\ \\ \\begin{bmatrix}\n",
    "\\beta_{0} + \\beta_{1} x_{1}^{(0)} + \\beta_{2} x_{2}^{(0)} \\\\ \n",
    "\\beta_{0} + \\beta_{1} x_{1}^{(1)} + \\beta_{2} x_{2}^{(1)} \\\\ \n",
    "\\vdots \\\\ \n",
    "\\beta_{0} + \\beta_{1} x_{1}^{(n)} + \\beta_{2} x_{2}^{(n)}\n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOhTq4w4zOvg"
   },
   "source": [
    "Run the following cell after you have implemented the `__call__` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6CUOmMAzOvg"
   },
   "outputs": [],
   "source": [
    "LRv1_model_core = LinearRegression_v1(2, seed = 42)\n",
    "LRv1_model_wrapper = ModelWrapper(LRv1_model_core)\n",
    "print(LRv1_model_wrapper)\n",
    "\n",
    "X_checkoff = np.array([[1, 1], [2, 3], [1.5, 2]])\n",
    "y_checkoff = LRv1_model_wrapper.predict(X_checkoff)\n",
    "print(f\"\\n\", f\"outputs_checkoff = \\n{y_checkoff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYgdeaO3zOvg"
   },
   "source": [
    "The results from the previous cell should look like this:\n",
    "```\n",
    "wrapper around the following model:\n",
    "linear regression model\n",
    " - coefficients = [[ 0.304717]\n",
    "                   [-1.039984]]\n",
    " - bias = 0.750451\n",
    " \n",
    "outputs_checkoff = \n",
    "[[ 0.01518417]\n",
    " [-1.76006696]\n",
    " [-0.8724414 ]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iu5cidQTRLMt"
   },
   "source": [
    "Now, the model has not been trained yet. However, before we go out training the model, we need a loss function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19CUWuJNzOvg"
   },
   "source": [
    "### Linear Regression Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-JoDeKQEXjb-"
   },
   "source": [
    "In the case of linear regression, the default choice of loss function is the mean squared error. In our case, it can be expressed as the following. \n",
    "\n",
    "\\begin{align*}\n",
    "L(h) &= \\mathbb{E}[(y - h(\\mathbf{x}))^2] \\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^{n}{\\big(y^{(i)} - (\\beta_{0} + \\beta_{1} x_{1}^{(i)} + \\beta_{2} x_{2}^{(i)})\\big)^2}\n",
    "\\end{align*}\n",
    "\n",
    "The frequentist approach to finding the set of paremeters ($\\beta_0$, $\\beta_1$, and $\\beta_2$) is to find the minimum point of the empirical loss function, where the gradient will be the zero vector. \n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla{L(h)} &= \n",
    "(\\frac{\\partial L(h)}{\\partial \\beta_0}, \\,\n",
    "\\frac{\\partial L(h)}{\\partial \\beta_1}, \\,\n",
    "\\frac{\\partial L(h)}{\\partial \\beta_2}) \\\\\n",
    "&= (0,\\, 0,\\, 0)\n",
    "\\end{align*}\n",
    "\n",
    "In this lab, we will find the minimum point **without setting the partial derivaties to zero and solving them directly**. Instead, we will use an iterative method to gradually get closer to the minimum, which is called the **gradient descent** method.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaE1WFXQzOvh"
   },
   "source": [
    "Here is a visualization of the loss function for you. We couldn't really visualize all four dimensions of $(L,\\, \\beta_{0},\\, \\beta_{1},\\, \\beta_{2})$. By the limit of our biology and our flat computer screens, we can only see in 3D. So, we decided to visualize the 3D intersection of the 4D hyperparabola where $\\beta_{0}$ is set to be its optimal value (which we computed for you, so don't worry about it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBI4MLVRzOvh"
   },
   "outputs": [],
   "source": [
    "b1_mesh, b2_mesh, mse_mesh = lab1utils.get_loss_mesh(X, y_true, bias_optimized)\n",
    "fig_loss = lab1utils.visualize_loss_function(b1_mesh, b2_mesh, mse_mesh)\n",
    "fig_loss.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22BX-O1NB_58",
    "tags": []
   },
   "source": [
    "### Optimizing via Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV-1xX3II-XN"
   },
   "source": [
    "The supervised machine learning objective we've been discussing involves minimizing the loss evaluation for the data we are training on. We haven't coded up a routine for that yet though. In our case, the prediction function is $h(\\mathbf{x}) = \\beta_{0} + \\beta_{1} x_{1} + \\beta_{1} x_{2}$, but we have not yet found the set of parameters $(\\beta_{0},\\, \\beta_{1},\\, \\beta_{2})$ that minimizes the loss function, and that's where gradient descent chimes in. \n",
    "\n",
    "Obviously, we want to shift our weights towards a good configuration. One pretty instinctive approach is to shift them in a way that makes the loss decrease. This is the idea behind gradient descent. The idea here is that we can change the weights in such a way that we incrementally minimize the loss evaluation. \n",
    "\n",
    "- **Gradient:** Generalization of slope into higher dimensions. It's the rate of change when going in a speciifc direction.\n",
    "- **Descent:** To go down. In this case, this is referring to the gradient of the loss function. \n",
    "\n",
    "How can we do that? **With some calculus**!\n",
    "\n",
    "1. We begin by randomly initializing the parameters.\n",
    "\n",
    "$$\\beta_{j} = \\beta_{j, \\text{initial}}, \\quad j \\in \\{0, 1, 2\\}$$ \n",
    "\n",
    "2. We update the parameters with the gradients of the loss function.\n",
    "\n",
    "$$\\beta_{j, \\text{new}} = \\beta_{j, \\text{old}} - \\eta \\frac{\\partial L}{\\partial \\beta_{j}}, \\quad j \\in \\{0, 1, 2\\}$$\n",
    "\n",
    "3. Repeat updating the parameters until the loss value converges to its minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl928I6TzOvh"
   },
   "source": [
    "Here, $\\eta$ is called the **learning rate**, which is a carefully chosen number to ensure that the paramters will converge to the minimum point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjU2oEbhzOvh"
   },
   "source": [
    "In our case, the gradient for the bias is the following.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(h)}{\\partial \\beta_{0}} &= \n",
    "\\frac{\\partial}{\\partial \\beta_{0}} \\mathbb{E}[(y - h(\\mathbf{x}))^2] \\\\\n",
    "&= \\frac{\\partial}{\\partial \\beta_{0}} \\left( \n",
    "\\frac{1}{n}\\sum_{i=1}^{n}{\\big(y^{(i)} - \\beta_{0} - \\beta_{1} x_{1}^{(i)} - \\beta_{2} x_{2}^{(i)}\\big)^2}\n",
    "\\right) \\\\\n",
    "&= - \\frac{2}{n} \\sum_{i=1}^{n} \\big( y^{(i)}  - {\\hat{y}}^{(i)} \\big)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9uurpK_zOvh"
   },
   "source": [
    "In our case, the gradients for the coefficients are the following.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L(h)}{\\partial \\beta_{1}}  \n",
    "&= \\frac{\\partial}{\\partial \\beta_{1}} \\left( \n",
    "\\frac{1}{n}\\sum_{i=1}^{n}{\\big(y^{(i)} - \\beta_{0} - \\beta_{1} x_{1}^{(i)} - \\beta_{2} x_{2}^{(i)}\\big)^2}\n",
    "\\right) \\\\\n",
    "&= - \\frac{2}{n} \\sum_{i=1}^{n} \\big( y^{(i)}  - {\\hat{y}}^{(i)} \\big) x_{1}^{(i)}\n",
    "\\\\\n",
    "\\frac{\\partial L(h)}{\\partial \\beta_{2}}  \n",
    "&= - \\frac{2}{n} \\sum_{i=1}^{n} \\big( y^{(i)}  - {\\hat{y}}^{(i)} \\big) x_{2}^{(i)}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7433r1XzOvi"
   },
   "source": [
    "#### [Check-off #3] Implement the gradient descent\n",
    "\n",
    "Implement the `get_gradients` method and the `gradient_descent` below. You can take a hint by looking at the `fit` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HupJtoR9J6wS"
   },
   "outputs": [],
   "source": [
    "class LinearRegression_v2(LinearRegression_v1):\n",
    "    # def __init__(self, n_vars, seed = None): ## Already Implemented in v1\n",
    "    #     self.coef\n",
    "    #     self.bias\n",
    "    # def __str__(self):                       ## Already Implemented in v1\n",
    "    # def __call__(self, X):                   ## Already Implemented in v1\n",
    "    \n",
    "    def get_gradients(self, X, y_true):\n",
    "        \"\"\"\n",
    "        - returns the gradients for the coefficients and the bias\n",
    "        - do not use a for-loop or a while-loop\n",
    "        - use only vectorized operations with NumPy\n",
    "          - use np.mean() and be careful with the axis of vectorization\n",
    "        - grad_coef should be a NumPy array of shape (n_vars, 1),\n",
    "          which is the same shape as self.coef\n",
    "        - grad_bias should be a np.float64 variable\n",
    "        \"\"\"\n",
    "        differences = y_true - self(X)\n",
    "        \n",
    "        ## TODO\n",
    "        grad_coef = ??\n",
    "        grad_bias = ??\n",
    "        return grad_coef, grad_bias\n",
    "    \n",
    "    def gradient_descent(self, grad_coef, grad_bias, learning_rate):\n",
    "        \"\"\"\n",
    "        - does not return anything. \n",
    "        - just updates self.coef and self.bias\n",
    "        \"\"\"\n",
    "        ## TODO\n",
    "        self.coef ??\n",
    "        self.bias ??\n",
    "    \n",
    "    def fit(self, X, y_true, \n",
    "                 epochs = 40, \n",
    "                 learning_rate = 0.05, \n",
    "                 print_frequency = 10):\n",
    "        \n",
    "        ## Keeping track of the trajectories\n",
    "        coef_trajectory = [self.coef.copy().reshape(-1,1)]\n",
    "        bias_trajectory = [self.bias.copy()]\n",
    "        loss_trajectory = [np.mean((y_true - self(X))**2)]\n",
    "        \n",
    "        for current_epoch in range(1, 1+epochs):\n",
    "            ## Main bulk\n",
    "            grad_coef, grad_bias = self.get_gradients(X, y_true)\n",
    "            self.gradient_descent(grad_coef, grad_bias, learning_rate)\n",
    "            \n",
    "            ## Keeping track of the trajectories\n",
    "            coef_trajectory += [self.coef.copy().reshape(-1,1)]\n",
    "            bias_trajectory += [self.bias.copy()]\n",
    "            loss_trajectory += [np.mean((y_true - self(X))**2)]\n",
    "\n",
    "            ## Printing out the progress\n",
    "            if current_epoch%print_frequency == 0:\n",
    "                print(f\"epoch = {current_epoch}, loss = {loss_trajectory[-1]:.6f}\")          \n",
    "            \n",
    "        trajectories = (\n",
    "            np.stack(coef_trajectory),  ## Convert list of many np.arrays into\n",
    "            np.stack(bias_trajectory),  ##   a single large np.array \n",
    "            np.array(loss_trajectory)   ## Convert list into np.array\n",
    "        )  \n",
    "\n",
    "        return trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNV6TH15zOvi"
   },
   "source": [
    "Run the cell below after implmenting the gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beoUzEsjzOvi"
   },
   "outputs": [],
   "source": [
    "LRv2_model_core = LinearRegression_v2(2, seed = 42)\n",
    "LRv2_model_wrapper = ModelWrapper(LRv2_model_core)\n",
    "print(LRv2_model_wrapper)\n",
    "\n",
    "gradients_initial = LRv2_model_wrapper.model.get_gradients(X, y_true)\n",
    "\n",
    "print(\"\")\n",
    "print(f\"gradients on coefficients: \\n{gradients_initial[0]}\")\n",
    "print(f\"gradient on bias: {gradients_initial[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKK_wy0szOvi"
   },
   "source": [
    "If everything is correct, the result should look like this.\n",
    "\n",
    "```\n",
    "wrapper around the following model:\n",
    "linear regression model\n",
    " - coefficients = [[ 0.304717]\n",
    "                   [-1.039984]]\n",
    " - bias = 0.750451\n",
    "\n",
    "gradients on coefficients: \n",
    "[[ -57.7751128]\n",
    " [-100.8285632]]\n",
    "gradient on bias: -43.28252455275603\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQzUWlkTzOvi"
   },
   "source": [
    "#### [Check-off #4] Size of learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vqd3S1x_zOvi"
   },
   "source": [
    "Repeat runing the next cell multiple times with different values of `learning_rate` to visualize its effect. Again, we cannot really visualize 4D, so we decided to visualize the 3D intersection of the 4D hyperparabola where $\\beta_{0}$ is already optimal. Then, discuss the following cases with the TA.\n",
    "\n",
    "1. What happens when the learning rate is too small, like `learning_rate = 0.001`?\n",
    "2. What happens when the learning rate is too big, like `learning_rate = 0.072`?\n",
    "3. How should we find the perfect learning rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jeacD7bVzOvj"
   },
   "outputs": [],
   "source": [
    "# Let's re-initialize the model every time you run the cell again\n",
    "LRv2_model_core = LinearRegression_v2(2, seed = 42)\n",
    "LRv2_model_wrapper = ModelWrapper(LRv2_model_core)\n",
    "\n",
    "# for the purpose of visualization\n",
    "# beta_0 is set, so only beta_1 and beta_2 are changing\n",
    "LRv2_model_wrapper.model.bias = bias_optimized\n",
    "\n",
    "# Training the model\n",
    "coef_trajectory, bias_trajectory, loss_trajectory = \\\n",
    "    LRv2_model_wrapper.train(X, y_true, \n",
    "                             epochs = 40, \n",
    "                             learning_rate = 0.05, ## try different values for the learning rate\n",
    "                             print_frequency = 10)\n",
    "\n",
    "## Visualizing the model\n",
    "b1_mesh, b2_mesh, mse_mesh = lab1utils.get_loss_mesh(X, y_true, bias_optimized)\n",
    "fig_gradients = lab1utils.visualize_loss_function(b1_mesh, b2_mesh, mse_mesh)\n",
    "go_gradients, go_gradients_2D = lab1utils.get_gradients_go(coef_trajectory, loss_trajectory, \n",
    "                                                           offset=0.5)\n",
    "fig_gradients.add_trace(go_gradients)\n",
    "fig_gradients.add_trace(go_gradients_2D)\n",
    "fig_gradients.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9gzsiv9zOvj"
   },
   "source": [
    "### Visualize the Training Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ga2ewjgTzOvj"
   },
   "source": [
    "Finally, you can see how the model gets updated at every epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09yfKxLvzOvj"
   },
   "outputs": [],
   "source": [
    "# re-initialize the model\n",
    "LRv2_model_core = LinearRegression_v2(2, seed = 42)\n",
    "LRv2_model_wrapper = ModelWrapper(LRv2_model_core)\n",
    "print(LRv2_model_wrapper)\n",
    "\n",
    "# train the model again\n",
    "coef_trajectory, bias_trajectory, loss_trajectory = \\\n",
    "    LRv2_model_wrapper.train(X, y_true, \n",
    "                             epochs = 40, \n",
    "                             learning_rate = 0.05, ## try different values for the learning rate\n",
    "                             print_frequency = 10)\n",
    "\n",
    "# visualize\n",
    "fig_process = lab1utils.visualize_trajectory(X, y_true, coef_trajectory, bias_trajectory)\n",
    "fig_process.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7aY-3-aJ6wT"
   },
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t7IqO9itJ6wT"
   },
   "source": [
    "Some of you have probably noticed that **we have not divided up our data in this lab**. In many machine learning applications, you evaluate your model with the set of data that has not been used in the training process. So you would typically divide your data into the training set and the testing set, and you would use the training set to optimize the model parameters and the testing set to evaluate the model's performance. If your model has some hyperparameters, then you would divide up the data into the training set, validation set, and the testing set, and you would use the validation set to fit the hyperparameters. \n",
    "\n",
    "We didn't divide up the dataset in this lab, **because we wanted this lab to be a baby step for you**. However, starting from the first homework assignment, you will have separate training set and testing set.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zusm8nw_QpZK"
   },
   "source": [
    "# **2470 Section**: Implementing Logistic Regression From Scratch\n",
    "\n",
    "**This section is optional for 1470 students.**\n",
    "\n",
    "We have explored linear regression to make predictions on a continuous dependent variable. Now, we will look at a classification problem where our dependent variable is discrete or categorical. We will make predictions with the logistic regression model.\n",
    "\n",
    "##Classification: Linear Regression vs Logistic Regression\n",
    "\n",
    "Linear regressions makes predictions on a continuos range. For example, if we\n",
    "want to predict sunny or rainy weather, to use the linear regression model, we would have to convert this categories into numerical identifiers (1 for rain, 0 for sun). Could you identify a problem with using linear regression in this case? How could we interpret a predicted weather outcome of 0.7? How about 1.5?\n",
    "\n",
    "For binary classification, we need an ML model that gives us a bound outcome between 0 and 1 that we can interpret as the probablility of a target event. This is not possible by fitting a linear function to the data. \n",
    "\n",
    "##Sigmoid function\n",
    "\n",
    "Logistic regression is centered around the logistic function that describes\n",
    "the probability of event $x$ based on the trainable parameters $\\beta_{0}$ and $\\beta_{1}$.\n",
    "\n",
    "$$p\\left(x\\right) = \\frac{1}{1 + e^{-\\left(\\beta_{0} + \\beta_{1}x\\right)}}$$\n",
    "\n",
    "## Binary Cross-Entropy Loss\n",
    "\n",
    "Going back to the weather example (1 for rain, 0 for sun), we want from our model:\n",
    "\n",
    "  * $p(x)$ to be close to 1 as possible when it rains. Ideally:\n",
    "  $$\\Pi_{y_{i} = 1} p\\left(x_{i}\\right) = 1$$\n",
    "  * $1 - p(x)$ to be close to 1 as possible when it is sunny.\n",
    "  $$\\Pi_{y_{i} = 0} \\left(1 - p\\left(x_{i}\\right)\\right) = 1$$\n",
    "\n",
    "Then, we want to maximize:\n",
    "\n",
    "$$\\Pi_{y_{i} = 1} \\space p\\left(x_{i}\\right)  \\times  \\Pi_{y_{i} = 0} \\left(1 - p\\left(x_{i}\\right)\\right)$$\n",
    "\n",
    "This is called **Maximum Likelihood Estimation**, and it is the method used to calculate the parameters for a logistic regression function. Note that it is very similar to log-loss or cross-entropy loss. In that case, we include a negative sign in front of the expression because we want to **minimize loss** (as opposed to maximize likelihood of the prediction based on the data).\n",
    "\n",
    "We rewrite the maximum likelihood expression to arrive to the following formula:\n",
    "\n",
    "$$\\Pi_{s} \\space p\\left(x_{i}\\right) ^{y_{i}} \\times \\left(1 - p\\left(x_{i}\\right)\\right)^{1-y_{i}} = $$\n",
    "\n",
    "\n",
    "$$\\sum_{i=1}^{n} y_{i}\\log\\left(p\\left(x_{i}\\right)\\right) + (1-y_{i})\\log(1-p(x_{i}))$$\n",
    "\n",
    "The binary cross-entropy loss, then, seeks to maximize the likelihood of correct prediction given the data, by minimizing for all datapoints:\n",
    "\n",
    "$$-\\left(y_{i}\\log\\left(p\\left(x_{i}\\right)\\right) + (1-y_{i})\\log(1-p(x_{i}))\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebBbAN0MQrvE"
   },
   "source": [
    "## Implementing Logistic Regression\n",
    "\n",
    "In this lab, you will be implementing a logistic regression model to be trained on a breast cancer dataset provided by Sklearn. ([Sklearn]((https://https://scikit-learn.org/stable/getting_started.html/)) is a python library popular for machine learning and data analysis. It provides many basic machine learning models, sample dataset like MNIST, and other helper functions such as train-test splitor and model evaluators). You will use the binary cross-entropy as the loss function and train the model using gradient descent.\n",
    "\n",
    "One difference here is we want challenge you to derive the gradient formula yourself. Consult the *Optimizing via Gradient Descent* section of the lab for how to derive the gradients using calculus. We will ask you to show you work and explain the process during check-off as well, so **be prepared**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J_9hcKIHQsZM"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vTv7-9hpQuFs"
   },
   "outputs": [],
   "source": [
    "class Logistic_Regression():\n",
    "    def __init__(self, n_vars, seed = 42):\n",
    "        self.weights = np.zeros(n_vars).reshape(-1, 1)\n",
    "        self.bias = np.float64(0.0) \n",
    "\n",
    "        if seed:\n",
    "            rng = np.random.default_rng(seed = seed)\n",
    "            self.weights = rng.normal(self.weights, scale = 1.0)\n",
    "            self.bias = np.float64(rng.normal(self.bias, scale = 1.0))\n",
    "\n",
    "    def __call__(self, X):\n",
    "        ## TODO: Implement the call function\n",
    "        y_pred = ??\n",
    "        \n",
    "        return y_pred # the probability of y being 1\n",
    "      \n",
    "    def sigmoid(self, logit):\n",
    "        ## TODO: Implement sigmoid function\n",
    "        sigmoid = ??\n",
    "\n",
    "        return sigmoid\n",
    "\n",
    "    def loss(self, y_true, y_pred):\n",
    "        # TODO: Implement binary cross-entropy loss function.\n",
    "        # If you encounter \"log on zero\" error message, consider adding a constant epsilon inside the log functions\n",
    "        epsilon = 1e-9\n",
    "        loss = ??\n",
    "\n",
    "        return loss\n",
    "      \n",
    "    def accuracy(self, y_true, y_pred):\n",
    "      return np.sum(y_pred == y_true)/len(y_pred)\n",
    "\n",
    "    def get_gradients(self, X, y_true):\n",
    "        \"\"\"\n",
    "        Returns the gradients for the weights and the bias.\n",
    "        - do not use a for-loop or a while-loop\n",
    "        - use only vectorized operations with NumPy\n",
    "          - use np.mean() and be careful with the axis of vectorization\n",
    "        - grad_weights should be a NumPy array of shape (n_vars, 1),\n",
    "          which is the same shape as self.weights\n",
    "        - grad_bias should be a np.float64 variable\n",
    "        \"\"\"\n",
    "        # TODO: Derive the loss function with respect to parameter B_{1} \n",
    "        # to obtain the formula for the gradient of the weight. \n",
    "        # Then, return the mean of all gradients.\n",
    "\n",
    "        # Similarly, derive the loss function with respect to parameter B_{0}\n",
    "        # to obtain the formula for the gradient of the bias.\n",
    "        # Then, return the mean of all gradients.\n",
    "\n",
    "        # TODO:\n",
    "        grad_weights = ??\n",
    "        grad_bias = ??\n",
    "\n",
    "        return grad_weights, grad_bias\n",
    "    \n",
    "    def gradient_descent(self, grad_weights, grad_bias, learning_rate):\n",
    "        \"\"\" \n",
    "        Update the trainable parameters with the learning rate and the gradients.\n",
    "        \"\"\"\n",
    "        # TODO:\n",
    "        self.weights = ??\n",
    "        self.bias = ??\n",
    "        \n",
    "    def fit(self, X, y_true, \n",
    "                 epochs = 2000, \n",
    "                 learning_rate = 0.01, \n",
    "                 print_frequency = 100):\n",
    "        print(\"Start training of logistic regression model.\")\n",
    "          \n",
    "        for current_epoch in range(1, 1+epochs):\n",
    "            ## Main bulk\n",
    "            grad_coef, grad_bias = self.get_gradients(X, y_true)\n",
    "            self.gradient_descent(grad_coef, grad_bias, learning_rate)\n",
    "\n",
    "            if current_epoch%print_frequency == 0:\n",
    "                print(f\"\"\"epoch = {current_epoch}, loss = {self.loss(y_true,\n",
    "                          self.__call__(X))}, accuracy = {self.accuracy(\n",
    "                              y_true, self.__call__(X))}\"\"\")\n",
    "        print(\"End training of logistic regression model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f38rGF8eQvtt"
   },
   "outputs": [],
   "source": [
    "# Load and extract data.\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "# Divide data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35)\n",
    "\n",
    "dataset_size = X.shape[0]\n",
    "feature_size = X.shape[1]\n",
    "\n",
    "print(f\"number of data samples: {dataset_size}\")\n",
    "print(f\"number of features: {feature_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2u18EITtQxDB"
   },
   "outputs": [],
   "source": [
    "LogR_model = Logistic_Regression(feature_size)\n",
    "LogR_model.fit(X_train, y_train)\n",
    "print(f'Testing Accuracy Score: {accuracy_score(y_test, LogR_model(X_test).astype(int))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ym3m_m1ZR15q"
   },
   "source": [
    "#### [2470 (1470-optional) Check-off] You model should reach at least 80% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWSD4ps49E3r"
   },
   "source": [
    "## Acknowledgements & Sources\n",
    "\n",
    "This lab is originally written by HTA Vadim Kudlay with neural network models as the examples of machine learning models for the Spring 2022 Semester. Then, it was rewritten to use the linear regression model by TA Yeunun Choo and editted by HTA Vadim Kudlay for the Fall 2022 Semester. The lab was updated by\n",
    "TA Tanadol Lamlertprasertkul and TA Shirley Loayza Sanchez for the Fall 2022 semester.\n",
    "\n",
    "For more, please consult the textbook [The Elements of Statistical Learning](https://hastie.su.domains/ElemStatLearn/)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "iinFWrAiZyAW"
   ],
   "provenance": [
    {
     "file_id": "14tfi_cBf6WbOyKbS_TndU2WDSy1tzfNC",
     "timestamp": 1644199951186
    },
    {
     "file_id": "1zIb5V6IzI9Az1BZF-bAqDapl7HsyWRgm",
     "timestamp": 1642610445037
    },
    {
     "file_id": "1Edvzv_LeUDD9IZp6e7BHpcbWqSVXe9vo",
     "timestamp": 1601247936712
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
